{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f10735-05e9-4cf0-8f77-f2e4aa572b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      " - C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model\\dim_season.csv\n",
      " - C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model\\dim_team.csv\n",
      " - C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model\\dim_match.csv\n",
      " - C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model\\fact_team_match.csv\n",
      "\n",
      "Counts:\n",
      "dim_season: 4\n",
      "dim_team: 26\n",
      "dim_match: 1520\n",
      "fact_team_match: 3040\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 0) Paths (robust if you run from /notebooks)\n",
    "# -------------------------\n",
    "cwd = Path.cwd()\n",
    "PROJECT_ROOT = cwd.parent if cwd.name.lower() == \"notebooks\" else cwd\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\"\n",
    "OUT_DIR = DATA_PROCESSED / \"sql_model\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_PATH = DATA_PROCESSED / \"pl_master_21-25_v1.csv\"\n",
    "if not MASTER_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Master file not found: {MASTER_PATH.resolve()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load master\n",
    "# -------------------------\n",
    "df = pd.read_csv(MASTER_PATH)\n",
    "\n",
    "# CORRECTION: Replace the vertical bar with an underscore in match_id.\n",
    "df['match_id'] = df['match_id'].str.replace('|', '_', regex=False)\n",
    "\n",
    "# Ensure Date is parsed consistently (your data is already yyyy-mm-dd, this just makes it explicit)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) dim_season\n",
    "# -------------------------\n",
    "seasons = sorted(df[\"Season\"].dropna().unique())\n",
    "dim_season = pd.DataFrame({\"season\": seasons})\n",
    "dim_season[\"season_id\"] = range(1, len(dim_season) + 1)\n",
    "dim_season = dim_season[[\"season_id\", \"season\"]]\n",
    "\n",
    "season_map = dict(zip(dim_season[\"season\"], dim_season[\"season_id\"]))\n",
    "\n",
    "# -------------------------\n",
    "# 3) dim_team (union across all relevant team columns)\n",
    "# -------------------------\n",
    "team_cols = [\"Team\", \"home_team\", \"away_team\", \"opponent\"]\n",
    "all_teams = pd.unique(pd.concat([df[c].dropna().astype(str) for c in team_cols], ignore_index=True))\n",
    "all_teams = sorted(all_teams)\n",
    "\n",
    "dim_team = pd.DataFrame({\"team_name\": all_teams})\n",
    "dim_team[\"team_id\"] = range(1, len(dim_team) + 1)\n",
    "dim_team = dim_team[[\"team_id\", \"team_name\"]]\n",
    "\n",
    "team_map = dict(zip(dim_team[\"team_name\"], dim_team[\"team_id\"]))\n",
    "\n",
    "# -------------------------\n",
    "# 4) dim_match (1 row per match_id)\n",
    "# -------------------------\n",
    "dim_match = (\n",
    "    df[[\"match_id\", \"Season\", \"Date\", \"home_team\", \"away_team\", \"home_goals\", \"away_goals\"]]\n",
    "    .drop_duplicates(\"match_id\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "dim_match[\"season_id\"] = dim_match[\"Season\"].map(season_map).astype(\"int64\")\n",
    "dim_match[\"home_team_id\"] = dim_match[\"home_team\"].astype(str).map(team_map).astype(\"int64\")\n",
    "dim_match[\"away_team_id\"] = dim_match[\"away_team\"].astype(str).map(team_map).astype(\"int64\")\n",
    "\n",
    "dim_match[\"match_date\"] = dim_match[\"Date\"].dt.date\n",
    "dim_match[\"match_label\"] = (\n",
    "    dim_match[\"home_team\"].astype(str)\n",
    "    + \" \"\n",
    "    + dim_match[\"home_goals\"].astype(str)\n",
    "    + \"-\"\n",
    "    + dim_match[\"away_goals\"].astype(str)\n",
    "    + \" \"\n",
    "    + dim_match[\"away_team\"].astype(str)\n",
    ")\n",
    "\n",
    "dim_match = dim_match[\n",
    "    [\"match_id\", \"season_id\", \"match_date\", \"home_team_id\", \"away_team_id\", \"home_goals\", \"away_goals\", \"match_label\"]\n",
    "].sort_values([\"season_id\", \"match_date\", \"match_id\"])\n",
    "\n",
    "# -------------------------\n",
    "# 5) fact_team_match (1 row per team per match)\n",
    "# -------------------------\n",
    "fact = df.copy()\n",
    "\n",
    "fact[\"season_id\"] = fact[\"Season\"].map(season_map).astype(\"int64\")\n",
    "fact[\"team_id\"] = fact[\"Team\"].astype(str).map(team_map).astype(\"int64\")\n",
    "fact[\"opponent_team_id\"] = fact[\"opponent\"].astype(str).map(team_map).astype(\"int64\")\n",
    "\n",
    "# Convert boolean to int (SQL-friendly)\n",
    "fact[\"is_home\"] = fact[\"is_home\"].astype(int)\n",
    "\n",
    "# Drop redundant string columns that will live in dimensions\n",
    "# (keep match_id + keys + metrics)\n",
    "drop_cols = [\"Season\", \"Team\", \"opponent\", \"home_team\", \"away_team\", \"Match\", \"Date\"]\n",
    "fact = fact.drop(columns=[c for c in drop_cols if c in fact.columns])\n",
    "\n",
    "# Reorder: keys first\n",
    "key_cols = [\"match_id\", \"season_id\", \"team_id\", \"opponent_team_id\", \"is_home\"]\n",
    "other_cols = [c for c in fact.columns if c not in key_cols]\n",
    "fact = fact[key_cols + other_cols]\n",
    "\n",
    "# -------------------------\n",
    "# 6) Quick validations vs master expectations\n",
    "# -------------------------\n",
    "assert len(df) == 3040, f\"Expected 3040 rows in master, got {len(df)}\"\n",
    "assert len(dim_match) == 1520, f\"Expected 1520 unique matches, got {len(dim_match)}\"\n",
    "assert len(fact) == 3040, f\"Fact should have same rows as master, got {len(fact)}\"\n",
    "assert fact[[\"match_id\", \"team_id\"]].duplicated().sum() == 0, \"Duplicate (match_id, team_id) found in fact!\"\n",
    "\n",
    "# -------------------------\n",
    "# 7) Save outputs (CSV) to load into SQL easily\n",
    "# -------------------------\n",
    "dim_season.to_csv(OUT_DIR / \"dim_season.csv\", index=False, encoding=\"utf-8\")\n",
    "dim_team.to_csv(OUT_DIR / \"dim_team.csv\", index=False, encoding=\"utf-8\")\n",
    "dim_match.to_csv(OUT_DIR / \"dim_match.csv\", index=False, encoding=\"utf-8\")\n",
    "fact.to_csv(OUT_DIR / \"fact_team_match.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" -\", (OUT_DIR / \"dim_season.csv\").resolve())\n",
    "print(\" -\", (OUT_DIR / \"dim_team.csv\").resolve())\n",
    "print(\" -\", (OUT_DIR / \"dim_match.csv\").resolve())\n",
    "print(\" -\", (OUT_DIR / \"fact_team_match.csv\").resolve())\n",
    "\n",
    "print(\"\\nCounts:\")\n",
    "print(\"dim_season:\", len(dim_season))\n",
    "print(\"dim_team:\", len(dim_team))\n",
    "print(\"dim_match:\", len(dim_match))\n",
    "print(\"fact_team_match:\", len(fact))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5e1a06-3e0e-46b8-8a90-92b97e42b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model_snake\\dim_season.csv\n",
      "✅ saved: C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model_snake\\dim_team.csv\n",
      "✅ saved: C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model_snake\\dim_match.csv\n",
      "✅ saved: C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model_snake\\fact_team_match.csv\n",
      "\n",
      "Done. Use the CSVs from: C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\\data_processed\\sql_model_snake\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\tozes\\Documents\\IronHack\\pl_21-25_analysis\")\n",
    "IN_DIR = BASE / \"data_processed\" / \"sql_model\"\n",
    "OUT_DIR = BASE / \"data_processed\" / \"sql_model_snake\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- files ---\n",
    "FILES = {\n",
    "    \"dim_season\": \"dim_season.csv\",\n",
    "    \"dim_team\": \"dim_team.csv\",\n",
    "    \"dim_match\": \"dim_match.csv\",\n",
    "    \"fact_team_match\": \"fact_team_match.csv\",\n",
    "}\n",
    "\n",
    "# --- mapping only for columns that need changing (spaces, hyphens, case) ---\n",
    "RENAME = {\n",
    "    # core metrics\n",
    "    \"xG\": \"xg\",\n",
    "    \"xGA\": \"xga\",\n",
    "    \"xGD\": \"xgd\",\n",
    "    \"Open Play xG\": \"open_play_xg\",\n",
    "    \"Open Play xGA\": \"open_play_xga\",\n",
    "    \"Open Play xGD\": \"open_play_xgd\",\n",
    "    \"Set Piece xG\": \"set_piece_xg\",\n",
    "    \"Set Piece xGA\": \"set_piece_xga\",\n",
    "    \"Set Piece xGD\": \"set_piece_xgd\",\n",
    "    \"npxG\": \"npxg\",\n",
    "    \"npxGA\": \"npxga\",\n",
    "    \"npxGD\": \"npxgd\",\n",
    "    \"Goals\": \"goals\",\n",
    "    \"Goals Conceded\": \"goals_conceded\",\n",
    "    \"GD\": \"gd\",\n",
    "    \"GD-xGD\": \"gd_minus_xgd\",\n",
    "    \"Possession\": \"possession\",\n",
    "    \"Field Tilt\": \"field_tilt\",\n",
    "    \"Avg Pass Height\": \"avg_pass_height\",\n",
    "    \"xT\": \"xt\",\n",
    "    \"xT Against\": \"xt_against\",\n",
    "    \"Passes in Opposition Half\": \"passes_in_opposition_half\",\n",
    "    \"Passes into Box\": \"passes_into_box\",\n",
    "    \"Shots\": \"shots\",\n",
    "    \"Shots Faced\": \"shots_faced\",\n",
    "    \"Shots per 1.0 xT\": \"shots_per_1_0_xt\",\n",
    "    \"Shots Faced per 1.0 xT Against\": \"shots_faced_per_1_0_xt_against\",\n",
    "    \"PPDA\": \"ppda\",\n",
    "    \"High Recoveries\": \"high_recoveries\",\n",
    "    \"High Recoveries Against\": \"high_recoveries_against\",\n",
    "    \"Crosses\": \"crosses\",\n",
    "    \"Corners\": \"corners\",\n",
    "    \"Fouls\": \"fouls\",\n",
    "    \"On-Ball Pressure\": \"on_ball_pressure\",\n",
    "    \"On-Ball Pressure Share\": \"on_ball_pressure_share\",\n",
    "    \"Off-Ball Pressure\": \"off_ball_pressure\",\n",
    "    \"Off-Ball Pressure Share\": \"off_ball_pressure_share\",\n",
    "    \"Game Control\": \"game_control\",\n",
    "    \"Game Control Share\": \"game_control_share\",\n",
    "    \"Throw-Ins into the Box\": \"throw_ins_into_box\",\n",
    "}\n",
    "\n",
    "def save_snake(name: str):\n",
    "    in_path = IN_DIR / FILES[name]\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    # rename columns (only those in RENAME)\n",
    "    df = df.rename(columns=RENAME)\n",
    "\n",
    "    # optional: force all columns to lower snake when they are already safe\n",
    "    # (uncomment if you want: team_id -> team_id stays the same)\n",
    "    # df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    out_path = OUT_DIR / FILES[name]\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅ saved: {out_path}\")\n",
    "\n",
    "for t in FILES:\n",
    "    save_snake(t)\n",
    "\n",
    "print(\"\\nDone. Use the CSVs from:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19140ab-82a5-431a-8134-e61c179af8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Scraping)",
   "language": "python",
   "name": "scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
